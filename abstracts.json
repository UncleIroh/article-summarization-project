[
  {
    "url": "https://fermatslibrary.com/s/parasitic-infection-increases-risk-taking-in-a-social-intermediate-host-carnivore#email-newsletter",
    "text": "Toxoplasma gondii is a protozoan parasite capable of infecting any warm-blooded species and can increase risk-taking in intermediate hosts. Despite extensive laboratory research on theeffects of T. gondii infection on behaviour, little is understood about the effects of toxoplasmosis on wild intermediate host behavior. Yellowstone National Park, Wyoming, USA,has a diverse carnivore community including gray wolves (Canis lupus) and cougars (Puma concolor), intermediate and deﬁnitive hosts of T. gondii, respectively. Here, we used 26 years of wolf behavioural, spatial, and serological data to show that wolf territory overlap with areasof high cougar density was an important predictor of infection. In addition, seropositivewolves were more likely to make high-risk decisions such as dispersing and becoming a packleader, both factors critical to individual ﬁtness and wolf vital rates. Due to the social hierarchy within a wolf pack, we hypothesize that the behavioural effects of toxoplasmosis maycreate a feedback loop that increases spatial overlap and disease transmission between wolves and cougars. These ﬁndings demonstrate that parasites have important implications for intermediate hosts, beyond acute infections, through behavioural impacts. Particularly in asocial species, these impacts can surge beyond individuals to affect groups, populations, and even ecosystem processes",
    "category": "Biology"
  },
  {
    "url": "https://www.sciencedirect.com/science/article/pii/S1319157820303554",
    "text": "The scientific research process generally starts with the examination of the state of the art, which may involve a vast number of publications. Automatically summarizing scientific articles would help researchers in their investigation by speeding up the research process. The automatic summarization of scientific articles differs from the summarization of generic texts due to their specific structure and inclusion of citation sentences. Most of the valuable information in scientific articles is presented in tables, figures, and algorithm pseudocode. These elements, however, do not usually appear in a generic text. Therefore, several approaches that consider the particularity of a scientific article structure were proposed to enhance the quality of the generated summary, resulting in ad hoc automatic summarizers. This paper provides a comprehensive study of the state of the art in this field and discusses some future research directions. It particularly presents a review of approaches developed during the last decade, the corpora used, and their evaluation methods. It also discusses their limitations and points out some open problems. The conclusions of this study highlight the prevalence of extractive techniques for the automatic summarization of single monolingual articles using a combination of statistical, natural language processing, and machine learning techniques. The absence of benchmark corpora and gold standard summaries for scientific articles remains the main issue for this task.",
    "category": "Science"
  },
  {
    "url": "https://dl.acm.org/doi/10.1145/3517745.3561450",
    "text": "Online services such as Facebook and Google serve as a popular way by which users today are exposed to products, services, viewpoints, and opportunities. These services implement advertising platforms that enable precise targeting of platform users, and they optimize the delivery of ads to the subset of the targeted users predicted to be most receptive. Unfortunately, recent work has shown that such delivery can---often without the advertisers' knowledge---show ads to biased sets of users based only on the content of the ad. Such concerns are particularly acute for ads that contain pictures of people (e.g., job ads showing workers), as advertisers often select images to carefully convey their goals and values (e.g., to promote diversity in hiring). However, it remains unknown how ad delivery algorithms react to---and make delivery decisions based on---demographic features of people represented in such ad images. Here, we examine how one major advertising platform (Facebook) delivers ads that include pictures of people of varying ages, genders, and races. We develop techniques to isolate the effect of these demographic variables, using a combination of both stock photos and realistic synthetically-generated images of people. We find dramatic skews in who ultimately sees ads solely based on the demographics of the person in the ad. Ads are often delivered disproportionately to users similar to those pictured: images of Black people are shown more to Black users, and the age of the person pictured correlates positively with the age of the users to whom it is shown. But, this is not universal, and more complex effects emerge: older women see more images of children, while images of younger women are shown disproportionately to men aged 55 and older. These findings bring up novel technical, legal, and policy questions and underscore the need to better understand how platforms deliver ads today.",
    "category": "Computer Science"
  },
  {
    "url": "https://dl.acm.org/doi/10.1145/3555143",
    "text": "Increasing demands for fact-checking have led to a growing interest in developing systems and tools to automate the fact-checking process. However, such systems are limited in practice because their system design often does not take into account how fact-checking is done in the real world and ignores the insights and needs of various stakeholder groups core to the fact-checking process. This paper unpacks the fact-checking process by revealing the infrastructures---both human and technological---that support and shape fact-checking work. We interviewed 26 participants belonging to 16 fact-checking teams and organizations with representation from 4 continents. Through these interviews, we describe the human infrastructure of fact-checking by identifying and presenting, in-depth, the roles of six primary stakeholder groups, 1) Editors, 2) External fact-checkers, 3) In-house fact-checkers, 4) Investigators and researchers, 5) Social media managers, and 6) Advocators. Our findings highlight that the fact-checking process is a collaborative effort among various stakeholder groups and associated technological and informational infrastructures. By rendering visibility to the infrastructures, we reveal how fact-checking has evolved to include both short-term claims centric and long-term advocacy centric fact-checking. Our work also identifies key social and technical needs and challenges faced by each stakeholder group. Based on our findings, we suggest that improving the quality of fact-checking requires systematic changes in the civic, informational, and technological contexts.",
    "category": "Computer Science"
  },
  {
    "url": "https://dl.acm.org/doi/10.1145/3542929.3563482",
    "text": "Production incidents in today's large-scale cloud services can be extremely expensive in terms of customer impacts and engineering resources required to mitigate them. Despite continuous reliability efforts, cloud services still experience severe incidents due to various root-causes. Worse, many of these incidents last for a long period as existing techniques and practices fail to quickly detect and mitigate them. To better understand the problems, we carefully study hundreds of recent high severity incidents and their postmortems in Microsoft-Teams, a large-scale distributed cloud based service used by hundreds of millions of users. We answer: (a) why the incidents occurred and how they were resolved, (b) what the gaps were in current processes which caused delayed response, and (c) what automation could help make the services resilient. Finally, we uncover interesting insights by a novel multi-dimensional analysis that correlates different troubleshooting stages (detection, root-causing and mitigation), and provide guidance on how to tackle complex incidents through automation or testing at different granularity.",
    "category": "Computer Science"
  },
  {
    "url": "https://www.aeaweb.org/aer/top20/35.4.519-530.pdf",
    "text": "What is the problem we wish to solve when we try to construct a rational economic order? On certain familiar assumptions the answer is simple enough. If we possess all the relevant information, if we can start out from a given system of preferences and if we command complete knowledge of available means, the problem which remains is purely one of logic. That is, the answer to the question of what is the best use of the available means is implicit in our assumptions. The conditions which the solution of this optimum problem must satisfy have been fully worked out and can be stated best in mathematical form: put at their briefest, they are that the marginal rates of substitution between any two commodities or factors must be the same in all their different uses. This, however, is emphatically not the economic problem which society faces. And the economic calculus which we have developed to solve this logical problem, though an important step toward the solution of the economic problem of society, does not yet provide an answer to it. The reason for this is that the 'data' from which the economic calculus starts are never for the whole society  'given ' to a single mind which could work out the implications, and can never be so given. The peculiar character of the problem of a rational economic order is determined precisely by the fact that the knowledge of the circumstances of which we must make use never exists in concentrated or integrated form, but solely as the dispersed bits of incomplete and frequently contradictory knowledge which all the separate individuals possess. The economic problem of society is thus not merely a problem of how to allocate  'given ' resources-if ''given' is taken to mean given to a single mind which deliberately solves the problem set by these  'data. ' It is rather a problem of how to secure the best use of resources known to any of the members of society, for ends whose relative importance only these individuals know. Or, to put it briefly, it is a problem of the utilization of knowledge not given to anyone in its totality.",
    "category": "Economics"
  },
  {
    "url": "https://assets.aeaweb.org/asset-server/files/9442.pdf",
    "text": "This paper is an exploratory and tentative study of the specific differentia of medical care as the object of normative economics. It is contended here, on the basis of comparison of obvious characteristics of the medical-care industry with the norms of welfare economics, that the special economic problems of medical care can be explained as adaptations to the existence of uncertainty in the incidence of disease and in the efficacy of treatment. It should be noted that the subject is the medical-care industry, not health. The causal factors in health are many, and the provision of medical care is only one. Particularly at low levels of income, other commoditiesuch as nutrition, shelter, clothing, and sanitation may be much more significant. It is the complex of services that center about the physician, private and group practice, hospitals, and public health, which I propose to discuss. The focus of discussion will be on the way the operation of the medical-care industry and the efficacy with which it satisfies the needs of society differ from a norm, if at all. The 'norm' that the economist usually uses for the purposes of such comparisons is the operation of a competitive model, that is, the flows of services that would be offered and purchased and the prices that would be paid for them if each individual in the market offered or purchased services at the going prices as if his decisions had no influence over them, and the going prices were such that the amounts of services which were available equalled the total amounts which other individuals were willing to purchase, with no imposed restrictions on supply or demand.",
    "category": "Economics"
  },
  {
    "url": "https://www.aeaweb.org/aer/top20/58.1.1-17.pdf",
    "text": "There is wide agreement about the major goals of economic policy: high employment, stable prices, and rapid growth. There is less agreement that these goals are mutually compatible or, among those who regard them as incompatible, about the terms at which they can and should be substituted for one another. There is least agreement about the role that various instruments of policy can and should play in achieving the several goals. My topic for tonight is the role of one such instrument-monetary policy. What can it contribute? And how should it be conducted to contribute the most? Opinion on these questions has fluctuated widely. In the first flush of enthusiasm about the newly created Federal Reserve System, many observers attributed the relative stability of the 1920s to the System's capacity for fine tuning-to apply an apt modern term. It came to be widely believed that a new era had arrived in which business cycles had been rendered obsolete by advances in monetary technology. This opinion was shared by economist and layman alike, though, of course, there were some dissonant voices. The Great Contraction destroyed this naive attitude. Opinion swung to the other extreme. Monetary policy was a string. You could pull on it to stop inflation but you could not push on it to halt recession. You could lead a horse to water but you could not make him drink. Such theory by aphorism was soon replaced by Keynes' rigorous and sophisticated analysis. Keynes offered simultaneously an explanation for the presumed impotence of monetary policy to stem the depression, a nonmonetary interpretation of the depression, and an alternative to monetary policy for meeting the depression and his offering was avidly accepted. If liquidity preference is absolute or nearly so-as Keynes believed likely in times of heavy unemployment-interest rates cannot be lowered by monetary measures. If investment and consumption are little affected by interest rates-as Hansen and many of Keynes' other American disciples came to believe-lower interest rates, even if they could be achieved, would do little good. Monetary policy is twice damned. The contraction, set in train, on this view, by a collapse of investment or by a shortage of investment opportunities or by stubborn thriftiness, could not, it was argued, have been stopped by monetary measures. But there was available an alternative-fiscal policy. Government spending could make up for insufficient private investment. Tax reductions could undermine stubborn thriftiness. ",
    "category": "Economics"
  },
  {
    "url": "https://www.nature.com/articles/s41467-021-25236-9",
    "text": "With the current interest in cultured meat, mammalian cell-based meat has mostly been unstructured. There is thus still a high demand for artificial steak-like meat. We demonstrate in vitro construction of engineered steak-like tissue assembled of three types of bovine cell fibers (muscle, fat, and vessel). Because actual meat is an aligned assembly of the fibers connected to the tendon for the actions of contraction and relaxation, tendon-gel integrated bioprinting was developed to construct tendon-like gels. In this study, a total of 72 fibers comprising 42 muscles, 28 adipose tissues, and 2 blood capillaries were constructed by tendon-gel integrated bioprinting and manually assembled to fabricate steak-like meat with a diameter of 5 mm and a length of 10 mm inspired by a meat cut. The developed tendon-gel integrated bioprinting here could be a promising technology for the fabrication of the desired types of steak-like cultured meats.",
    "category": "Biology"
  },
  {
    "url": "https://www.nature.com/articles/s41467-021-25348-2",
    "text": "Intricate color patterns are a defining aspect of morphological diversity in the Felidae. We applied morphological and single-cell gene expression analysis to fetal skin of domestic cats to identify when, where, and how, during fetal development, felid color patterns are established. Early in development, we identify stripe-like alterations in epidermal thickness preceded by a gene expression pre-pattern. The secreted Wnt inhibitor encoded by Dickkopf 4 plays a central role in this process, and is mutated in cats with the Ticked pattern type. Our results bring molecular understanding to how the leopard got its spots, suggest that similar mechanisms underlie periodic color pattern and periodic hair follicle spacing, and identify targets for diverse pattern variation in other mammals.",
    "category": "Biology"
  }
]
